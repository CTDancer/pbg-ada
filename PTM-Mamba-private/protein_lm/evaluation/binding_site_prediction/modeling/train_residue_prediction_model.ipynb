{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from icecream import ic\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import ast\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AC_ID</th>\n",
       "      <th>wt_seq</th>\n",
       "      <th>ptm_seq</th>\n",
       "      <th>pdb_id_with_chain_name</th>\n",
       "      <th>aligned_labels_with_gaps</th>\n",
       "      <th>aligned_labels</th>\n",
       "      <th>labels</th>\n",
       "      <th>ppbs_seq_alignment</th>\n",
       "      <th>ptm_seq_alignment</th>\n",
       "      <th>is_test</th>\n",
       "      <th>is_val</th>\n",
       "      <th>is_train</th>\n",
       "      <th>esm_650m_embedding_path</th>\n",
       "      <th>esm_650m_embedding_padding_mask_path</th>\n",
       "      <th>mamba_with_ptms_embedding_path</th>\n",
       "      <th>mamba_with_ptms_embedding_padding_mask_path</th>\n",
       "      <th>mamba_without_ptms_embedding_path</th>\n",
       "      <th>mamba_without_ptms_embedding_padding_mask_path</th>\n",
       "      <th>ppbs_embedding_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C4YMW2</td>\n",
       "      <td>MSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPSNPS...</td>\n",
       "      <td>MSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPSNPS...</td>\n",
       "      <td>4esw_A</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>GSHMSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPS...</td>\n",
       "      <td>---MSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPS...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P97291</td>\n",
       "      <td>MPERLAETLMDLWTPLIILWITLPSCVYTAPMNQAHVLTTGSPLEL...</td>\n",
       "      <td>MPERLAETLMDLWTPLIILWITLPSCVYTAPMNQAHVLTTGSPLEL...</td>\n",
       "      <td>1zxk_A</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>------------------------S---------------------...</td>\n",
       "      <td>MPERLAETLMDLWTPLIILWITLPSCVYTAPMNQAHVLTTGSPLEL...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P93114</td>\n",
       "      <td>MAVPMDTISGPWGNNGGNFWSFRPVNKINQIVISYGGGGNNPIALT...</td>\n",
       "      <td>M&lt;N-acetylalanine&gt;VPMDTISGPWGNNGGNFWSFRPVNKINQ...</td>\n",
       "      <td>1ouw_B</td>\n",
       "      <td>[-1, -1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, ...</td>\n",
       "      <td>--VPMDTISGPWGNNGGNFWSFRPVNKINQIVISYGGGGNNPIALT...</td>\n",
       "      <td>MAVPMDTISGPWGNNGGNFWSFRPVNKINQIVISYGGGGNNPIALT...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P42262</td>\n",
       "      <td>MQKIMHISVLLSPVLWGLIFGVSSNSIQIGGLFPRGADQEYSAFRV...</td>\n",
       "      <td>MQKIMHISVLLSPVLWGLIFGVSSNSIQIGGLFPRGADQEYSAFRV...</td>\n",
       "      <td>2xhd_A</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>------------------------N---------------------...</td>\n",
       "      <td>MQKIMHISVLLSPVLWGLIFGVSSNSIQIGGLFPRGADQEYSAFRV...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O75208</td>\n",
       "      <td>MAAAAVSGALGRAGWRLLQLRCLPVARCRQALVPRAFHASAVGLRS...</td>\n",
       "      <td>MAAAAVSGALGRAGWRLLQLRCLPVARCRQALVPRAFHASAVGLRS...</td>\n",
       "      <td>4rhp_B</td>\n",
       "      <td>[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>----------------------------------------------...</td>\n",
       "      <td>MAAAAVSGALGRAGWRLLQLRCLPVARCRQALVPRAFHASAVGLRS...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "      <td>/workspace/protein_lm/evaluation/binding_site_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AC_ID                                             wt_seq  \\\n",
       "0  C4YMW2  MSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPSNPS...   \n",
       "1  P97291  MPERLAETLMDLWTPLIILWITLPSCVYTAPMNQAHVLTTGSPLEL...   \n",
       "2  P93114  MAVPMDTISGPWGNNGGNFWSFRPVNKINQIVISYGGGGNNPIALT...   \n",
       "3  P42262  MQKIMHISVLLSPVLWGLIFGVSSNSIQIGGLFPRGADQEYSAFRV...   \n",
       "4  O75208  MAAAAVSGALGRAGWRLLQLRCLPVARCRQALVPRAFHASAVGLRS...   \n",
       "\n",
       "                                             ptm_seq pdb_id_with_chain_name  \\\n",
       "0  MSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPSNPS...                 4esw_A   \n",
       "1  MPERLAETLMDLWTPLIILWITLPSCVYTAPMNQAHVLTTGSPLEL...                 1zxk_A   \n",
       "2  M<N-acetylalanine>VPMDTISGPWGNNGGNFWSFRPVNKINQ...                 1ouw_B   \n",
       "3  MQKIMHISVLLSPVLWGLIFGVSSNSIQIGGLFPRGADQEYSAFRV...                 2xhd_A   \n",
       "4  MAAAAVSGALGRAGWRLLQLRCLPVARCRQALVPRAFHASAVGLRS...                 4rhp_B   \n",
       "\n",
       "                            aligned_labels_with_gaps  \\\n",
       "0  [1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "2  [-1, -1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1...   \n",
       "3  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "4  [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...   \n",
       "\n",
       "                                      aligned_labels  \\\n",
       "0  [1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, ...   \n",
       "2  [1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  ppbs_seq_alignment  \\\n",
       "0  GSHMSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPS...   \n",
       "1  ------------------------S---------------------...   \n",
       "2  --VPMDTISGPWGNNGGNFWSFRPVNKINQIVISYGGGGNNPIALT...   \n",
       "3  ------------------------N---------------------...   \n",
       "4  ----------------------------------------------...   \n",
       "\n",
       "                                   ptm_seq_alignment  is_test  is_val  \\\n",
       "0  ---MSTNKITFLLNWEAAPYHIPVYLANIKGYFKDENLDIAILEPS...        0       1   \n",
       "1  MPERLAETLMDLWTPLIILWITLPSCVYTAPMNQAHVLTTGSPLEL...        0       0   \n",
       "2  MAVPMDTISGPWGNNGGNFWSFRPVNKINQIVISYGGGGNNPIALT...        1       0   \n",
       "3  MQKIMHISVLLSPVLWGLIFGVSSNSIQIGGLFPRGADQEYSAFRV...        0       0   \n",
       "4  MAAAAVSGALGRAGWRLLQLRCLPVARCRQALVPRAFHASAVGLRS...        0       0   \n",
       "\n",
       "   is_train                            esm_650m_embedding_path  \\\n",
       "0         0  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "1         1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "2         0  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "3         1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "4         1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "\n",
       "                esm_650m_embedding_padding_mask_path  \\\n",
       "0  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "2  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "3  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "4  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "\n",
       "                      mamba_with_ptms_embedding_path  \\\n",
       "0  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "2  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "3  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "4  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "\n",
       "         mamba_with_ptms_embedding_padding_mask_path  \\\n",
       "0  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "2  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "3  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "4  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "\n",
       "                   mamba_without_ptms_embedding_path  \\\n",
       "0  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "2  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "3  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "4  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "\n",
       "      mamba_without_ptms_embedding_padding_mask_path  \\\n",
       "0  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "1  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "2  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "3  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "4  /workspace/protein_lm/evaluation/binding_site_...   \n",
       "\n",
       "                                 ppbs_embedding_path  \n",
       "0  /workspace/protein_lm/evaluation/binding_site_...  \n",
       "1  /workspace/protein_lm/evaluation/binding_site_...  \n",
       "2  /workspace/protein_lm/evaluation/binding_site_...  \n",
       "3  /workspace/protein_lm/evaluation/binding_site_...  \n",
       "4  /workspace/protein_lm/evaluation/binding_site_...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residue_seqs_csv_path = '/workspace/protein_lm/evaluation/binding_site_prediction/data/residue_seqs_processed.csv'\n",
    "residue_seqs_df = pd.read_csv(residue_seqs_csv_path)\n",
    "residue_seqs_df['aligned_labels_with_gaps'] = residue_seqs_df['aligned_labels_with_gaps'].apply(ast.literal_eval)\n",
    "residue_seqs_df['aligned_labels'] = residue_seqs_df['aligned_labels'].apply(ast.literal_eval)\n",
    "residue_seqs_df['labels'] = residue_seqs_df['labels'].apply(ast.literal_eval)\n",
    "residue_seqs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| max_seq_len: 6486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6486"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max(residue_seqs_df['wt_seq'].apply(len))\n",
    "ic(max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResiduePredictionDataset(Dataset):\n",
    "\tdef __init__(self, df, embedding_column, padding_mask_column, max_seq_len, cutoff_seq_len=1000, label_column='aligned_labels'):\n",
    "\t\tself.df = df\n",
    "\t\tself.embedding_column = embedding_column\n",
    "\t\tself.padding_mask_column = padding_mask_column\n",
    "\t\tself.max_seq_len = max_seq_len\n",
    "\t\tself.cutoff_seq_len = cutoff_seq_len\n",
    "\t\tself.label_column = label_column\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.df)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\trow = self.df.iloc[idx]\n",
    "\t\tppbs_seq = row['ppbs_seq_alignment'].replace('-', '')\n",
    "\t\tembedding_path = row[self.embedding_column]\n",
    "\t\tembedding = torch.load(embedding_path)\n",
    "\t\tlabels = torch.tensor(row[self.label_column], dtype=torch.float32)\n",
    "\t\t# ic(embedding.shape, labels.shape, len(ppbs_seq))\n",
    "\t\t# ic(embedding, labels, ppbs_seq)\n",
    "\t\treturn {'embedding': embedding, 'labels': labels}\n",
    "\n",
    "\n",
    "test_dataset = ResiduePredictionDataset(residue_seqs_df, 'esm_650m_embedding_path', 'esm_650m_embedding_padding_mask_path', max_seq_len)\n",
    "\n",
    "embedding, labels = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def crop_seq(input_ids, max_seq_len):\n",
    "    \"\"\"\n",
    "    randomly crop sequences to max_seq_len\n",
    "    Args:\n",
    "        input_ids: tensor of shape (seq_len)\n",
    "        max_seq_len: int\n",
    "    \"\"\"\n",
    "    seq_len = len(input_ids)\n",
    "    if seq_len <= max_seq_len:\n",
    "        return input_ids\n",
    "    else:\n",
    "        start_idx = torch.randint(0, seq_len - max_seq_len + 1, (1,)).item()\n",
    "        return input_ids[start_idx : start_idx + max_seq_len]\n",
    "def collate_fn(batch):\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    min_seq_len = min([len(label) for label in labels])\n",
    "    embeddings = [item['embedding'] for item in batch]\n",
    "    embeddings = torch.stack([crop_seq(embedding, min_seq_len) for embedding in embeddings])\n",
    "\n",
    "    labels = torch.stack([crop_seq(label, min_seq_len) for label in labels])\n",
    "    # labels = pad_sequence(\n",
    "    #         labels,\n",
    "    #         batch_first=True,\n",
    "    #         padding_value=-1,\n",
    "    # )\n",
    "    # embeddings = pad_sequence(\n",
    "    #     embeddings, \n",
    "    #     batch_first=True, \n",
    "    #     padding_value=0.0  # Assuming 0.0 is an appropriate padding value for your embeddings\n",
    "    # )\n",
    "    pad_mask = (labels != -1)\n",
    "    return embeddings, labels, pad_mask\n",
    "    # embeddings is a list of tensors, each of shape (seq_len, embedding_dim) please pad them to the same length\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(train_df): 2544, len(val_df): 263, len(test_df): 243\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2544, 263, 243)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = residue_seqs_df[residue_seqs_df['is_train'] == True]\n",
    "val_df = residue_seqs_df[residue_seqs_df['is_val'] == True]\n",
    "test_df = residue_seqs_df[residue_seqs_df['is_test'] == True]\n",
    "\n",
    "ic(len(train_df), len(val_df), len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "esm_train_dataset = ResiduePredictionDataset(train_df, 'esm_650m_embedding_path', 'esm_650m_embedding_padding_mask_path', max_seq_len)\n",
    "esm_val_dataset = ResiduePredictionDataset(val_df, 'esm_650m_embedding_path', 'esm_650m_embedding_padding_mask_path', max_seq_len)\n",
    "esm_test_dataset = ResiduePredictionDataset(test_df, 'esm_650m_embedding_path', 'esm_650m_embedding_padding_mask_path', max_seq_len)\n",
    "\n",
    "mamba_wt_train_dataset = ResiduePredictionDataset(train_df, 'mamba_without_ptms_embedding_path', 'mamba_without_ptms_embedding_padding_mask_path', max_seq_len)\n",
    "mamba_wt_val_dataset = ResiduePredictionDataset(val_df, 'mamba_without_ptms_embedding_path', 'mamba_without_ptms_embedding_padding_mask_path', max_seq_len)\n",
    "mamba_wt_test_dataset = ResiduePredictionDataset(test_df, 'mamba_without_ptms_embedding_path', 'mamba_without_ptms_embedding_padding_mask_path', max_seq_len)\n",
    "\n",
    "mamba_ptm_train_dataset = ResiduePredictionDataset(train_df, 'mamba_with_ptms_embedding_path', 'mamba_with_ptms_embedding_padding_mask_path', max_seq_len)\n",
    "mamba_ptm_val_dataset = ResiduePredictionDataset(val_df, 'mamba_with_ptms_embedding_path', 'mamba_with_ptms_embedding_padding_mask_path', max_seq_len)\n",
    "mamba_ptm_test_dataset = ResiduePredictionDataset(test_df, 'mamba_with_ptms_embedding_path', 'mamba_with_ptms_embedding_padding_mask_path', max_seq_len)\n",
    "\n",
    "ppbs_train_dataset = ResiduePredictionDataset(train_df, 'ppbs_embedding_path', '', max_seq_len, label_column='labels')\n",
    "ppbs_val_dataset = ResiduePredictionDataset(val_df, 'ppbs_embedding_path', '', max_seq_len, label_column='labels')\n",
    "ppbs_test_dataset = ResiduePredictionDataset(test_df, 'ppbs_embedding_path', '', max_seq_len, label_column='labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "esm_train_loader = DataLoader(esm_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "esm_val_loader = DataLoader(esm_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "esm_test_loader = DataLoader(esm_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "mamba_wt_train_loader = DataLoader(mamba_wt_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "mamba_wt_val_loader = DataLoader(mamba_wt_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "mamba_wt_test_loader = DataLoader(mamba_wt_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "mamba_ptm_train_loader = DataLoader(mamba_ptm_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "mamba_ptm_val_loader = DataLoader(mamba_ptm_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "mamba_ptm_test_loader = DataLoader(mamba_ptm_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "ppbs_train_loader = DataLoader(ppbs_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "ppbs_val_loader = DataLoader(ppbs_val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "ppbs_test_loader = DataLoader(ppbs_test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| embedding.shape: torch.Size([96, 1280])\n",
      "    labels.shape: torch."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Size([96])\n",
      "ic| embedding: tensor([[ 0.0678,  0.1634, -0.0771,  ...,  0.1612, -0.0652, -0.0625],\n",
      "                       [-0.0928, -0.0198, -0.1412,  ..., -0.0006, -0.1817, -0.0120],\n",
      "                       [-0.0697,  0.1247, -0.0773,  ...,  0.0706, -0.0806, -0.0698],\n",
      "                       ...,\n",
      "                       [-0.1904,  0.1109,  0.0012,  ..., -0.3720, -0.0700,  0.0319],\n",
      "                       [ 0.1291,  0.0144,  0.2626,  ...,  0.1445,  0.0790, -0.1674],\n",
      "                       [ 0.0236,  0.0195,  0.0934,  ..., -0.3019, -0.1133,  0.3432]])\n",
      "    labels: tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "                    1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                    0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "                    1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "                    0., 0., 1., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0678,  0.1634, -0.0771,  ...,  0.1612, -0.0652, -0.0625],\n",
       "         [-0.0928, -0.0198, -0.1412,  ..., -0.0006, -0.1817, -0.0120],\n",
       "         [-0.0697,  0.1247, -0.0773,  ...,  0.0706, -0.0806, -0.0698],\n",
       "         ...,\n",
       "         [-0.1904,  0.1109,  0.0012,  ..., -0.3720, -0.0700,  0.0319],\n",
       "         [ 0.1291,  0.0144,  0.2626,  ...,  0.1445,  0.0790, -0.1674],\n",
       "         [ 0.0236,  0.0195,  0.0934,  ..., -0.3019, -0.1133,  0.3432]]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
       "         0., 0., 1., 0., 0., 0.]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element = next(iter(ppbs_train_dataset))\n",
    "embedding = element['embedding']\n",
    "labels = element['labels']\n",
    "\n",
    "ic(embedding.shape, labels.shape)\n",
    "ic(embedding, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = '/workspace/protein_lm/evaluation/binding_site_prediction/data/embeddings/A0A0B4J1L0_mamba_with_ptms.pt'\n",
    "embedding = torch.load(embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name  | Type  | Params\n",
      "--------------------------------\n",
      "0 | model | Model | 656 K \n",
      "--------------------------------\n",
      "656 K     Trainable params\n",
      "0         Non-trainable params\n",
      "656 K     Total params\n",
      "2.628     Total estimated model params size (MB)\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=255` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:  22%|██▎       | 9/40 [00:00<00:01, 21.01it/s, v_num=90, train_loss=0.457, train_acc=0.683]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.types import EVAL_DATALOADERS\n",
    "from sklearn.metrics import f1_score, matthews_corrcoef, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class Model(nn.Module):\n",
    "\tdef __init__(self, input_dim, hidden_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "\t\tself.linear2 = nn.Linear(hidden_dim, 2)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.linear1(x))\n",
    "\t\tx = self.linear2(x)\n",
    "\t\treturn x  # Removed softmax here\n",
    "\n",
    "class ResiduePredictionModel(pl.LightningModule):\n",
    "\tdef __init__(self, input_dim, hidden_dim, test_loader, lr=1e-3):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.model = Model(input_dim, hidden_dim)\n",
    "\t\tself.lr = lr\n",
    "\t\tself.test_loader = test_loader\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\tembeddings, labels, padding_mask = batch\n",
    "\t\tpadding_mask = padding_mask.to(torch.bool)\n",
    "\t\tlabels = labels.to(torch.long).unsqueeze(-1)\n",
    "\t\tlabels = labels.expand(-1, -1, 2)\n",
    "\t\tlabels[:, :, 1] = 1 - labels[:, :, 0]\n",
    "\t\tlabels = labels.float()\n",
    "\t\tpredictions = self.model(embeddings)\n",
    "\t\t# ic(labels.shape, labels.dtype) # (64, 38)\n",
    "\t\t# ic(predictions.shape, predictions.dtype) # (64, 38, 2)\n",
    "\t\t# ic(labels)\n",
    "\t\t# ic(predictions)\n",
    "\t\t# loss_fn = nn.BCEWithLogitsLoss()\n",
    "\t\t# loss = loss_fn(predictions, labels)\n",
    "\n",
    "\t\t# Corrected accuracy calculation\n",
    "\t\ttrue_labels = labels.argmax(dim=-1)\n",
    "\t\tpred_labels = predictions.argmax(dim=-1)\n",
    "\t\tacc = (true_labels == pred_labels).float().mean()\n",
    "\t\t# f1 = f1_score(labels.cpu().numpy(), predictions.argmax(dim=-1).cpu().numpy())\n",
    "\t\t# mcc = matthews_corrcoef(labels.cpu().numpy(), predictions.argmax(dim=-1).cpu().numpy())\n",
    "\t\t# recall = recall_score(labels.cpu().numpy(), predictions.argmax(dim=-1).cpu().numpy())\n",
    "\t\tself.log('train_loss', loss, prog_bar=True)\n",
    "\t\tself.log('train_acc', acc, prog_bar=True)\n",
    "\t\t# self.log('train_f1', f1, prog_bar=True)\n",
    "\t\t# self.log('train_mcc', mcc, prog_bar=True)\n",
    "\t\t# self.log('train_recall', recall, prog_bar=True)\n",
    "\t\treturn loss\n",
    "\t\n",
    "\tdef configure_optimizers(self):\n",
    "\t\treturn optim.Adam(self.parameters(), lr=self.lr)\n",
    "\t\n",
    "\t# def validation_step(self, batch, batch_idx):\n",
    "\t# \tic('validation step')\n",
    "\t# \tembeddings, labels, padding_mask = batch\n",
    "\t# \tpadding_mask = padding_mask.to(torch.bool)\n",
    "\t# \tlabels = labels.to(torch.long)\n",
    "\t# \tpredictions = self.model(embeddings)\n",
    "\t# \tmasked_predictions = predictions#[padding_mask]\n",
    "\t# \tmasked_labels = labels#[padding_mask]\n",
    "\t# \tloss = F.cross_entropy(masked_predictions, masked_labels)\n",
    "\t# \t# Corrected accuracy calculation\n",
    "\t# \taccuracy = (masked_predictions.argmax(dim=-1) == masked_labels).float().mean()\n",
    "\t# \tself.log('val_loss', loss, prog_bar=True)\n",
    "\t# \tself.log('val_accuracy', accuracy, prog_bar=True)\n",
    "\t# \tf1 = f1_score(masked_labels.cpu().numpy(), masked_predictions.argmax(dim=-1).cpu().numpy())\n",
    "\t# \tself.log('val_f1', f1, prog_bar=True)\n",
    "\t# \treturn loss\n",
    "\t\n",
    "\t# def test_step(self, batch, batch_idx):\n",
    "\t# \tembeddings, labels, padding_mask = batch\n",
    "\t# \tpadding_mask = padding_mask.to(torch.bool)\n",
    "\t# \tlabels = labels.to(torch.long)\n",
    "\t# \tpredictions = self.model(embeddings)\n",
    "\t# \tmasked_predictions = predictions[padding_mask]\n",
    "\t# \tmasked_labels = labels[padding_mask]\n",
    "\t# \tloss = F.cross_entropy(masked_predictions, masked_labels)\n",
    "\t# \t# Corrected accuracy calculation\n",
    "\t# \taccuracy = (masked_predictions.argmax(dim=-1) == masked_labels).float().mean()\n",
    "\t# \tf1 = f1_score(masked_labels.cpu().numpy(), masked_predictions.argmax(dim=-1).cpu().numpy())\n",
    "\t# \tself.log('test_loss', loss, prog_bar=True)\n",
    "\t# \tself.log('test_accuracy', accuracy, prog_bar=True)\n",
    "\t# \treturn loss\n",
    "\t\n",
    "\tdef test_dataloader(self):\n",
    "\t\treturn self.test_loader\n",
    "\t\t\n",
    "\n",
    "# Assuming esm_650m_embedding_dim, input_dim, hidden_dim, and learning rate are defined elsewhere\n",
    "esm_650m_embedding_dim = 1280\n",
    "input_dim = esm_650m_embedding_dim\n",
    "hidden_dim = 512\n",
    "model = ResiduePredictionModel(input_dim, hidden_dim, esm_test_loader, lr=2e-2)\n",
    "trainer = pl.Trainer(max_epochs=20, devices=[5],overfit_batches=0)  # Ensure overfit_batches is set appropriately\n",
    "# Assuming esm_train_loader and esm_val_loader are defined elsewhere\n",
    "trainer.fit(model, ppbs_train_loader, ppbs_val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
