## Disease and Druggability Benchmark
Phosphorylation site prediction

This codebase provides a script to train and evaluate protein sequence classification models using different embedding methods, including one-hot encoding, Mamba, and various ESM models. The pipeline allows you to specify which ESM versions to use for training and evaluation.

## Requirements

- Python 3.8+
- PyTorch
- PyTorch Lightning
- ESM
- Pandas
- scikit-learn


### Dataset
The dataset should be in CSV format with the following columns:

- uniprot_id
- accession_id
- wt_seq
- ptm_seq
- is_druggable
- is_disease
- is_part_of_druggability_dataset
- is_part_of_disease_dataset
- is_train

Place your dataset CSV file `seqs_df.csv` in the root directory of the repository.

### Usage

##### ESM Models
The available ESM models and their configurations are:

- esm2_t48_15B_UR50D: 48 layers, 15B parameters, 5120 embedding dimension
- esm2_t36_3B_UR50D: 36 layers, 3B parameters, 2560 embedding dimension
- esm2_t33_650M_UR50D: 33 layers, 650M parameters, 1280 embedding dimension
- esm2_t30_150M_UR50D: 30 layers, 150M parameters, 640 embedding dimension
- esm2_t12_35M_UR50D: 12 layers, 35M parameters, 480 embedding dimension
- esm2_t6_8M_UR50D: 6 layers, 8M parameters, 320 embedding dimension

#### Configs
The `config.py` script holds configurations for **training** and **plotting**. 

**Training configs:**
```python
# GPU settings
CUDA_VISIBLE_DEVICES="1"                                    # str to queue up GPUs. leave blank if you don't want to set this variable

# Models to benchmark (bool)
BENCHMARK_ESM={
    "esm2_t48_15B_UR50D": False,                            # True to benchmark ESM2-15B
    "esm2_t36_3B_UR50D": True,                              # True to benchmark ESM2-3B
    "esm2_t33_650M_UR50D": True,                            # True to benchmark ESM2-650M
    "esm2_t30_150M_UR50D": False,                           # True to benchmark ESM2-150M
    "esm2_t12_35M_UR50D": False,                            # True to benchmark ESM2-35M
    "esm2_t6_8M_UR50D": False                               # True to benchmark ESM2-8M
}
BENCHMARK_ONEHOT=True                                       # True to benchmark One-Hot Model
BENCHMARK_ONEHOTPTM=True                                    # True to benchmark One-Hot-PTM Model
BENCHMARK_MAMBA=True                                        # True to benchmark PTM-Mamba

# Number of replicates
N_REPLICATES = 5                                            # How many times to train each model

# Benchmark tests to perform
DRUGGABILITY=True                                           # True to perform Druggability benchmark
DISEASE=True                                                # True to perform Disease benchmark
```
**Summarization and plotting configs:**

You can generate two types of output files after training classification benchmarking files. 

(1) *Summary table*: includes Accuracy, Precision, Recall, F1, MCC, AUPRC, and AUROC, as well as standard deviations (SD) between replicates for each metric. 
(2) *AUROC and PR curves*: each replicate of model is evaluated on a held-out test set duiring training. These plots are created by averaging probabilities across replicates.

```python
# Druggability file paths
DRUGGABILITY_RESULTS_DIR = 'results/timestamp'         # path/to/results/dir/for/summary/and/plotting
DISEASE_RESULTS_DIR = 'results/timestamp'              # path/to/results/dir/for/summary/and/plotting
```

#### Training
The `train.py` script generates embeddings for the input sequences using the specified models from `config.py`. Then, it trains a classifier on the embeddings. Finally, it calls methods from `summary.py` and `plot.py` to automatically generate summary tables and plots for the models you just trained. The output is within disease_druggable_benchmark, and is structured as follows:

```
results/
└── timestamp/                                      
    ├── summary
    ├── test_metrics
    ├── test_probabilities
    ├── disease_AUROC_curve.png
    ├── disease_PR_curve.png
    ├── druggability_AUROC_curve.png
    ├── druggability_PR_curve.png
    └── train_settings.txt
```

- **timestamp**: a unique string encoding today's date and time (e.g. jan_1_1023).
- **summary**: a folder containing the results of `summary.py`, which is automatically run on the results generated by `train.py`
- **test_metrics**: a folder containing test metrics for each length bin, for each replicate of a model. Each model gets its own file (e.g. `disease_test_results_onehot.csv`). These files are summarized by `summary.py`. Columns: *Length,accuracy,precision,recall,f1,mcc,auroc,auprc,run*.
- **test_probabilities**: a folder containing test set probabilities for each item in the test set, for each replicate of a model. Each model gets its own file (e.g. `disease_test_probs_and_labels_onehot.csv`). These files are plotted by `plot.py`. Columns: *prob_0,prob_1,label,run* (prob_0 is probability of class 0; prob_1 is probability of class 1; label is the true label for this item; run is the model replicate number).
- **train_settings.txt**: stores your config settings for training. 
- all **png** files are curves generated by `plot.py`.

To run, enter in terminal:
```bash
python train.py
```
or, to run the (sometimes long) training process in the background:
```bash
nohup python train.py > train.out 2> train.err &
```

#### Summary
The `summary.py` script generates two summarization tables, one for each benchmark: Disease (`disease_summary.csv`) and Druggability (`druggable_summary.csv`). You can specify the results you would like to summarize in `config.py`. For DRUGGABILITY_RESULTS_DIR and DISEASE_RESULTS_DIR, please enter the outer directory (`results/timestamp`), and the code will extract results from the `results/timstamp/test_metrics` subdirectory for summarization.  

For each Model (e.g. ESM-2-650M, OneHot, PTM-Mamba), the summary table includes Accuracy, Precision, Recall, F1, MCC, AUPRC, and AUROC, as well as standard deviations (SD) between replicates for each metric. Results will be deposited in `results/timestamp/summary`.

To run, enter in terminal:
```bash
python summary.py
```

#### Plotting
The `plot.py` script generates two AUROC and two PR curves, one for each benchmark: Disease (`disease_AUROC_curve.png`, `disease_PR_curve.png`) and Druggability (`druggable_AUROC_curve.png`, `druggable_PR_curve.png`). For DRUGGABILITY_RESULTS_DIR and DISEASE_RESULTS_DIR, please enter the outer directory (`results/timestamp`), and the code will extract results from the `results/timstamp/test_probaiblities` subdirectory for plotting. The code will average probabilities across replicates for each model before plotting curves.   

To run, enter in terminal:
```bash
python plot.py
```

